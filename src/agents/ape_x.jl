using Distributed

addprocs(;exeflags="--project")

@everywhere include("src/agents/ape_x_actor.jl")
@everywhere using Dates
@everywhere using CuArrays
@everywhere include("src/agents/rainbow_agent.jl")

using CuArrays
CuArrays.allowscalar(false)
include("patch.jl")


@everywhere function Base.length(rr::RemoteChannel)
    rid = remoteref_id(rr)
    return if rr.where == myid()
        length(Distributed.lookup_ref(rid).c.data)
    else
        remotecall_fetch(rid->length(Distributed.lookup_ref(rid).c.data), rr.where, rid)
    end
end

@everywhere function train(
    params_from_learner,
    batch_sample_channel,
    replay_msg_channel
    ;sync_param_period=100,
    batch_size=512,
    seed=123)

    Random.seed!(seed)
    env = HanabiEnv(;seed=seed)

    n_colors, n_ranks, n_hands, n_players, n_actions = num_colors(env.game), num_ranks(env.game), hand_size(env.game), num_players(env.game), max_moves(env.game)

    learner = RainbowAgent(
        n_colors * n_ranks,
        env.observation_length,
        n_actions;
        batch_size=batch_size
    )

    learners = (learner, (RainbowAgent(learner) for _ in 1:n_players-1)...)

    i, t = 0, now()
    put!(params_from_learner, collect.(Tracker.data.(learner.params)))
    while true
        if i % sync_param_period == 0
            learner_params = collect.(Tracker.data.(learner.params))
            take!(params_from_learner)
            put!(params_from_learner, learner_params)
            @info "params updated, time spent for last $sync_param_period updates: $(now() - t), remaining samples $(length(batch_sample_channel))"
            t = now()
        end
        i += 1

        states, actions, rewards, isdone, next_states, next_legal_actions, inds = take!(batch_sample_channel)
        updated_priorities = train_once!(learner,  gpu(states), actions, gpu(rewards), gpu(isdone), gpu(next_states), gpu(next_legal_actions))
        put!(replay_msg_channel, (inds, updated_priorities))

        @info "$i batches trained"
    end
end

@everywhere function maintain_replay_buffer(
    replay_msg_channel,
    batch_sample_channel,
    feature_size,
    n_actions,
    n_step,
    γ
    ;n_samples=10,
    batch_size=512,
    min_replay_buffer_size=1024,
    replay_capacity=1000000,
)

    replay_buffer = HanabiPrioritizedBuffer(replay_capacity, feature_size, n_actions, n_step, γ)
    n_seen = 0

    i, t = 0, now()
    while true
        # consume experiences from actor
        data = take!(replay_msg_channel)
        if data isa Tuple{HanabiEpisodeBuffer, Vector{Float32}}
            experience_batch, priorities = data
            push!(replay_buffer, experience_batch, priorities)
            n_seen += length(priorities)

            @info "worker [$(myid())] generating batch data"
            # generate batch samples for learner

            for _ in 1:n_samples
                if length(replay_buffer) > min_replay_buffer_size
                    states, actions, rewards, isdone, next_states, next_legal_actions, inds = sample(replay_buffer, batch_size)
                    absolute_inds = [n_seen - (length(replay_buffer) - ind) for ind in inds]
                    put!(batch_sample_channel, (states, actions, rewards, isdone, next_states, next_legal_actions, absolute_inds))
                end
            end

        elseif data isa Tuple{Vector{Int64}, Vector{Float32}}

            # update priorities generated by learner
            inds, priorities = data
            for (ind, p) in zip(inds, priorities)
                current_ind = length(replay_buffer) - (n_seen - ind)
                if current_ind > 0
                    replay_buffer.sum_tree[current_ind] = p
                end
            end
        else
            error("unknonwn data")
        end

        @info "time spent: [$(now() - t)], replay_buffer status: n_seen experiences=$n_seen, current replay buffer length=$(length(replay_buffer)), remaining experiences=$(length(replay_msg_channel)) remaining batches=$(length(batch_sample_channel))"

        t = now()
        i += 1
    end
end

function initialize_actors(
    params_from_learner,
    replay_msg_channel
    ;n_phase=10000,
    actor_workers=workers(),
    max_steps_of_local_buffer=5000)
    for (i, pid) in enumerate(actor_workers)
        remote_do(run_actor, pid, [0, 0], replay_msg_channel, params_from_learner; n_phase=n_phase,max_steps_of_local_buffer=max_steps_of_local_buffer)
    end
end


function eval_learners(learners)
    env = HanabiEnv()
    eval_stats = []
    for _ in 1:100
        n, r = run_one_episode(env, learners; is_train=false)
        push!(eval_stats, (episode_length=n, episode_reward=r))
    end
    @info "=(------> evaluation info of iter $i " avg_length=mean(x -> x.episode_length, eval_stats) avg_rewards=mean(x -> x.episode_reward, eval_stats)
end

function run()
    env = HanabiEnv()
    n_step, γ = 1, 0.99

    actor_workers = workers()[3:end]
    replay_buffer_worker = workers()[2]
    learner_worker = workers()[1]


    batch_sample_channel = RemoteChannel(() -> Channel{Tuple{Array{Int32, 2}, Vector{Int32}, Vector{Int32}, Vector{Bool}, Array{Int32, 2}, Array{Float32, 2}, Vector{Int}}}(10000), replay_buffer_worker)
    replay_msg_channel = RemoteChannel(() -> Channel{Union{Tuple{HanabiEpisodeBuffer, Vector{Float32}}, Tuple{Vector{Int64}, Vector{Float32}}}}(10000), replay_buffer_worker)
    params_from_learner = RemoteChannel(() -> Channel{Vector{Array{Float32}}}(1), learner_worker)

    initialize_actors(params_from_learner, replay_msg_channel; actor_workers=actor_workers)
    remote_do(train, learner_worker, params_from_learner, batch_sample_channel, replay_msg_channel)
    remote_do(maintain_replay_buffer, replay_buffer_worker, replay_msg_channel, batch_sample_channel, env.observation_length, max_moves(env.game), n_step, γ)
end